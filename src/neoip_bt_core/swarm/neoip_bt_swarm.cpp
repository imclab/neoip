/*! \file
    \brief Class to handle the bt_swarm_t

*/

/* system include */
/* local include */
#include "neoip_bt_swarm.hpp"
#include "neoip_bt_swarm_full.hpp"
#include "neoip_bt_swarm_peersrc.hpp"
#include "neoip_bt_swarm_utmsg.hpp"
#include "neoip_bt_swarm_sched.hpp"
#include "neoip_bt_swarm_event.hpp"
#include "neoip_bt_swarm_helper.hpp"
#include "neoip_bt_swarm_resumedata.hpp"
#include "neoip_bt_handshake.hpp"
#include "neoip_bt_ecnx_vapi.hpp"
#include "neoip_bt_utmsg_piecewish.hpp"
#include "neoip_bt_unit.hpp"
#include "neoip_bt_session.hpp"
#include "neoip_log.hpp"
#include "neoip_nipmem_alloc.hpp"


NEOIP_NAMESPACE_BEGIN

///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
//                    CTOR/DTOR
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

/** \brief Constructor
 */
bt_swarm_t::bt_swarm_t()		throw()
{
	// zero some fields
	bt_session	= NULL;
	m_pselect_vapi	= NULL;
	m_swarm_peersrc	= NULL;
	m_swarm_utmsg	= NULL;
	m_swarm_sched	= NULL;
}

/** \brief Destructor
 */
bt_swarm_t::~bt_swarm_t()		throw()
{
	// unlink this object from the bt_session
	if( bt_session )	bt_session->swarm_unlink(this);
	// close all pending bt_swarm_full_t
	while( !full_db.empty() )	nipmem_delete full_db.front();
	// sanity check - the ecnx_vapi_db MUST be empty at this point
	// - dont delete them as they are not owned by this object
	DBG_ASSERT( ecnx_vapi_db.empty() );

	// delete the bt_swarm_sched_t if needed
	nipmem_zdelete	m_swarm_sched;
	// delete the bt_swarm_peersrc_t if needed
	nipmem_zdelete	m_swarm_peersrc;
	// delete the bt_swarm_utmsg_t if needed
	nipmem_zdelete	m_swarm_utmsg;
	// delete the bt_pselect_vapi_t if needed
	nipmem_zdelete	m_pselect_vapi;
}


///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
//                         setup function
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

/** \brief Set the profile for this object
 */
bt_swarm_t &	bt_swarm_t::set_profile(const bt_swarm_profile_t &m_profile)throw()
{
	// sanity check - the profile MUST be checked OK
	DBG_ASSERT( m_profile.check() == bt_err_t::OK );	
	// copy the parameter
	this->m_profile	= m_profile;
	// return the object iself
	return *this;
}

/** \brief Start the operation
 * 
 * - a bt_swarm_t is always started with a bt_swarm_resumedata_t
 *   - even if it is the firstboot. in this case the bt_swarm_resumedata_t
 *     is generated by the bt_swarm_resumedata_t::from_mfile() function
 */
bt_err_t bt_swarm_t::start(bt_session_t *bt_session, const bt_swarm_resumedata_t &swarm_resumedata
		, bt_io_vapi_t *m_io_vapi, bt_swarm_cb_t *callback, void *userptr)	throw()
{
	bt_err_t	bt_err;
	// sanity check - the bt_swarm_resumedata_t MUST be check ok or null
	DBG_ASSERT( swarm_resumedata.check().succeed() );
	// sanity check - the bt_mfile_t MUST be fully_init
	DBG_ASSERT( swarm_resumedata.bt_mfile().is_fully_init() );
	// copy the parameter
	this->bt_session	= bt_session;
	this->bt_mfile		= swarm_resumedata.bt_mfile();
	this->m_io_vapi		= m_io_vapi;
	this->callback		= callback;
	this->userptr		= userptr;
	// sanity check - this infohash MUST be unique per bt_session_t
	DBG_ASSERT( !bt_session->swarm_by_infohash(bt_mfile.infohash()) );
	// link this object to the bt_session
	bt_session->swarm_dolink(this);

	// create the bt_pselect_vapi_t depending on the bt_swarm_profile_t::pselect_policy
	// - TODO should it be constructed externally ?
	//   - exactly like bt_io_vapi_t
	if( profile().pselect_policy() == bt_pselect_policy_t::SLIDE ){
		m_pselect_vapi	= nipmem_new bt_pselect_slide_t(bt_mfile.nb_piece());
	}else{
		// sanity check - the bt_pselect_policy_t MUST be a bt_pselect_fixed_t
		DBG_ASSERT( profile().pselect_policy() == bt_pselect_policy_t::FIXED );
		// use a bt_pselect_fixed_t
		m_pselect_vapi	= nipmem_new bt_pselect_fixed_t(bt_mfile.nb_piece());
		// set bt_pieceprec_arr_t IIF avail in swarm_resumedata anduses bt_io_sfile_t
		if( !swarm_resumedata.pieceprec_arr().empty() && bt_swarm_helper_t::use_io_sfile(this) ){
			bt_pselect_fixed_t *	pselect_fixed	= bt_swarm_helper_t::get_pselect_fixed(this);
			pselect_fixed->pieceprec_arr(swarm_resumedata.pieceprec_arr());
		}
	}
	
	// if a previous bt_swarm_resumedata_t is available, use it
	if( !swarm_resumedata.pieceavail_local().is_null() )
		pselect_vapi()->mark_isavail(swarm_resumedata.pieceavail_local());

	// get the partavail_piece_inval from the swarm_resumedata if available
	file_size_inval_t partavail_piece_inval;
	if( !swarm_resumedata.partavail_piece_inval().empty() )
		partavail_piece_inval = swarm_resumedata.partavail_piece_inval();

	// copy the bt_swarm_stats_t from the bt_swarm_resumedata_t
	m_swarm_stats	= swarm_resumedata.swarm_stats();
		
	// start the bt_swarm_sched_t
	m_swarm_sched	= nipmem_new bt_swarm_sched_t();
	bt_err		= swarm_sched()->set_profile(profile().sched())
					.start(this, partavail_piece_inval);
	if( bt_err.failed() )	return bt_err;

	// start the bt_swarm_peersrc_t -- with bt_peersrc_peer_arr_t from bt_swarm_resumedata_t
	m_swarm_peersrc	= nipmem_new bt_swarm_peersrc_t();
	bt_err		= swarm_peersrc()->start(this, swarm_resumedata.peersrc_peer_arr());
	if( bt_err.failed() )	return bt_err;

	// start the bt_swarm_utmsg_t
	m_swarm_utmsg	= nipmem_new bt_swarm_utmsg_t();
	bt_err		= m_swarm_utmsg->start(this);
	if( bt_err.failed() )	return bt_err;

	// return no error
	return bt_err_t::OK;
}

/** \brief Start the operation from a fully init bt_mfile_t
 * 
 * - TODO obsolete function - this function is just for compatibility
 *   - to be removed
 */
bt_err_t	bt_swarm_t::start(bt_session_t *bt_session, const bt_mfile_t &bt_mfile
				, bt_io_vapi_t *m_io_vapi, bt_swarm_cb_t *callback, void *userptr) throw()
{
	// forward to the 'read' start() with a bt_swarrm_resumedata_t
	return start(bt_session, bt_swarm_resumedata_t::from_mfile(bt_mfile), m_io_vapi, callback, userptr);
}

////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////
//                 query function
////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

/** \brief Return a bt_handshake_t for this bt_swarm_t
 */
bt_handshake_t	bt_swarm_t::get_handshake()	const throw()
{
	bt_handshake_t	bt_handshake;
	// build the bt_handshake_t
	bt_handshake.protocolid	( "BitTorrent protocol" );
	bt_handshake.protoflag	( bt_protoflag_t::UT_MSGPROTO );
	bt_handshake.infohash	( bt_mfile.infohash() );
	bt_handshake.peerid	( bt_session->local_peerid() );
	// return the just built bt_handshake_t
	return bt_handshake;
}

/** \brief Return the local peerid
 */
const bt_id_t &		bt_swarm_t::local_peerid()	const throw()
{
	return bt_session->local_peerid();
}

/** \brief Return the current bt_pieceavail_t for the piece locally fully available
 */
const bt_pieceavail_t &	bt_swarm_t::local_pavail()		const throw()
{
	return pselect_vapi()->local_pavail();
}

/** \brief Get the current bt_swarm_resumedata_t
 * 
 * - TODO should it save the bt_swarm_stats_t too ?
 *   - may be 2 level of stats, inter boot, intra boot
 * 
 */
bt_swarm_resumedata_t	bt_swarm_t::get_current_resumedata()	const throw()
{
	bt_swarm_resumedata_t	swarm_resumedata;
	// set the bt_mfile_t
	swarm_resumedata.bt_mfile(bt_mfile);
	// set the pieceavail_local
	// - the piece locally available: as it is useless to redownload them
	swarm_resumedata.pieceavail_local(local_pavail());
	// set the bt_pieceprec_arr_t
	// - NOTE: it is usefull only for in case of bt_pselect_fixed_t
	swarm_resumedata.pieceprec_arr(bt_swarm_helper_t::get_pieceprec_arr(this));
	// build the seer_arr from the currently established connections
	// - TODO should i put all the which which doesnt a fully established connection too ?
	// - yes and those should include the ttl and creation date of the bt_peersrc_peer_t
	// - seems close the peer exchange stuff
	bt_peersrc_peer_arr_t				peersrc_peer_arr;
	std::list<bt_swarm_full_t *>::const_iterator	iter;
	for(iter = full_db.begin(); iter != full_db.end(); iter++){
		bt_swarm_full_t *	swarm_full	= *iter;
		peersrc_peer_arr += swarm_full->peersrc_peer();
	}
	// set the peersrc_peer_arr
	swarm_resumedata.peersrc_peer_arr(peersrc_peer_arr);

	// set the partavail_piece_inval
	// - aka all the data already available in the incomplete pieces
	swarm_resumedata.partavail_piece_inval(swarm_sched()->get_partavail_piece_inval());
	// set the bt_swarm_stats_t
	swarm_resumedata.swarm_stats(swarm_stats());
	// specify that this bt_swarm_resumedata_t has already been allocated
	// - this is the case as it is required to do so before launch a bt_swarm_t 
	swarm_resumedata.mfile_allocated(true);
	// return the bt_swarm_resumedata_t
	return swarm_resumedata;
}

/** \brief Return the amount of data fully and partially available
 */
file_size_t	bt_swarm_t::totfile_anyavail()	const throw()
{
	file_size_t	totfile_anyavail(0);
	// update with the fully available piece
	totfile_anyavail	= bt_unit_t::totfile_avail(local_pavail(), bt_mfile);
	// update with partavail piece_inval from the bt_swarm_sched_t too
	totfile_anyavail	+= swarm_sched()->get_partavail_piece_inval().sum();
	// return the just computed value
	return totfile_anyavail;
}

////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////
//		query xmit/recv rate
////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

/** \brief Return the recv_rate for the bt_swarm_t
 */
double	bt_swarm_t::recv_rate()		const throw()
{
	double	total_recv_rate	= 0.0;
	// compute the recv_rate as the sum of the recv_rate_full() and recv_rate_ecnx()
	total_recv_rate	+= recv_rate_full();
	total_recv_rate	+= recv_rate_ecnx();
	// return the just computed receive rate
	return total_recv_rate;
}

/** \brief Return the recv_rate for all the bt_swarm_full_t
 */
double	bt_swarm_t::recv_rate_full()	const throw()
{
	double	total_recv_rate	= 0.0;
	// go thru the whole bt_swarm_t::full_db
	std::list<bt_swarm_full_t *>::const_iterator	iter;
	for(iter = full_db.begin(); iter != full_db.end(); iter++ ){
		const bt_swarm_full_t *	swarm_full	= *iter;
		// update the total_recv_rate
		total_recv_rate	+= swarm_full->recv_rate_avg();
	}
	// return the just computed value
	return total_recv_rate;	
}

/** \brief Return the recv_rate for all the bt_ecnx_vapi_t
 */
double	bt_swarm_t::recv_rate_ecnx()	const throw()
{
	double	total_recv_rate	= 0.0;
	// go thru the whole bt_swarm_t::ecnx_vapi_db
	bt_swarm_t::ecnx_vapi_db_t::const_iterator	iter;
	for( iter = ecnx_vapi_db.begin(); iter != ecnx_vapi_db.end(); iter++ ){
		const bt_ecnx_vapi_t *	ecnx_vapi	= iter->second;
		// update the total_recv_rate
		total_recv_rate	+= ecnx_vapi->recv_rate_avg();
	}
	// return the just computed value
	return total_recv_rate;
}

/** \brief Return the xmit_rate for the bt_swarm_t
 */
double	bt_swarm_t::xmit_rate()		const throw()
{
	double	total_xmit_rate	= 0.0;
	// compute the xmit_rate as the sum of the xmit_rate_full() and xmit_rate_ecnx()
	total_xmit_rate	+= xmit_rate_full();
	// NOTE: there is no xmit_rate_ecnx() because ecnx is download only
	// return the just computed receive rate
	return total_xmit_rate;
}

/** \brief Return the xmit_rate for all the bt_swarm_full_t
 */
double	bt_swarm_t::xmit_rate_full()	const throw()
{
	double	total_xmit_rate	= 0.0;
	// go thru the whole bt_swarm_t::full_db
	std::list<bt_swarm_full_t *>::const_iterator	iter;
	for(iter = full_db.begin(); iter != full_db.end(); iter++ ){
		const bt_swarm_full_t *	swarm_full	= *iter;
		// update the total_xmit_rate
		total_xmit_rate	+= swarm_full->xmit_rate_avg();
	}
	// return the just computed value
	return total_xmit_rate;	
}

////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////
//                   handle the full_db
////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

/** \brief Return a pointer on the bt_swarm_full_t matching this remote_addr, or NULL if none does
 */
bt_swarm_full_t	*bt_swarm_t::full_by_remote_addr(const socket_addr_t &remote_addr)	throw()
{
	std::list<bt_swarm_full_t *>::iterator	iter;
	// go thru the whole full_db
	for(iter = full_db.begin(); iter != full_db.end(); iter++){
		bt_swarm_full_t *	swarm_full	= *iter;
		// if this swarm_full remote_addr matches, return it
		if( remote_addr == swarm_full->remote_addr() )	return swarm_full;
	}
	// if this point is reached, no bt_swarm_full_t matches, so return NULL
	return NULL;
}


/** \brief Return a pointer on the bt_swarm_full_t matching this remote_peerid, or NULL if none does
 */
bt_swarm_full_t	*bt_swarm_t::full_by_remote_peerid(const bt_id_t &remote_peerid)	throw()
{
	std::list<bt_swarm_full_t *>::iterator	iter;
	// go thru the whole full_db
	for(iter = full_db.begin(); iter != full_db.end(); iter++){
		bt_swarm_full_t *	swarm_full	= *iter;
		// if this swarm_full remote_peerid matches, return it
		if( remote_peerid == swarm_full->remote_peerid() )	return swarm_full;
	}
	// if this point is reached, no bt_swarm_full_t matches, so return NULL
	return NULL;
}

/** \brief return true if it is allowed to create a new bt_swarm_full_t
 */
bool	bt_swarm_t::is_new_full_allowed()	const throw()
{
	// if current number of bt_swarm_full_t is >= than the no_new_full_limit, forbid any new one
	if( full_db.size() >= profile().no_new_full_limit() )	return false;
	// else allow it
	return true;
}

////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////
//                      handle the ecnx_vapi_db
////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

/** \brief DoLink this bt_ecnx_vapi_t to this bt_swarm_t
 */
void	bt_swarm_t::ecnx_vapi_dolink(bt_ecnx_vapi_t *ecnx_vapi)			throw()
{
	// add it to the ecnx_vapi_db
	bool succeed = ecnx_vapi_db.insert(std::make_pair(ecnx_vapi->get_object_slotid(), ecnx_vapi)).second;
	DBG_ASSERT( succeed );		
}

/** \brief UnLink this bt_ecnx_vapi_t to this bt_swarm_t
 */
void	bt_swarm_t::ecnx_vapi_unlink(bt_ecnx_vapi_t *ecnx_vapi)			throw()
{
	// sanity check - the bt_ecnx_vapi_t MUST be present in the ecnx_vapi_db
	DBG_ASSERT( ecnx_vapi_db.find(ecnx_vapi->get_object_slotid()) != ecnx_vapi_db.end() );
	// remove it from the ecnx_vapi_db
	ecnx_vapi_db.erase(ecnx_vapi->get_object_slotid());	
}

/** \brief Return the pointer on the bt_ecnx_vapi_t matching object_slotid, or NULL if none matches
 */
bt_ecnx_vapi_t *bt_swarm_t::ecnx_vapi_by_object_id(slot_id_t object_slotid)	throw()
{
	ecnx_vapi_db_t::iterator	iter;
	// try to find this object_slotid in the ecnx_vapi_db
	iter	= ecnx_vapi_db.find(object_slotid);
	// if none has been found, return NULL
	if( iter == ecnx_vapi_db.end() )	return NULL;
	// else return its bt_ecnx_vapi_t pointer
	return iter->second;
}


////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////
//			update function
////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

/** \brief this function notify the bt_swarm_t that the piece precedence have been changed
 */
void	bt_swarm_t::notify_pieceprec_change()	throw()
{
	// forward the notification to bt_swarm_sched_t
	swarm_sched()->notify_pieceprec_change();
	
	// forward to bt_utmsg_piecewish_t IIF supported by the local peer
	bt_utmsg_piecewish_t *	utmsg_piecewish	= swarm_utmsg()->utmsg_piecewish();
	if( utmsg_piecewish )	utmsg_piecewish->notify_pieceprec_change();
}

/** \brief called to notify a new piece is locally available
 * 
 * - NOTE: 
 *   - it may be called from the inside of bt_swarm_t when a new piece is available
 *   - it may be called from outside when data are taken from another mean
 *     and are made available to bt_swarm_t thru the bt_io_vapi_t
 * - NOTE: this function may delete the bt_swarm_t 
 *   - in fact this function notify the caller of the event and the caller
 *     may delete the bt_swarm_t
 * 
 * @return a tokeep for the bt_swarm_t
 */
bool	bt_swarm_t::declare_piece_newly_avail(size_t piece_idx)	throw()
{
	// sanity check - the piece MUST NOT be already available
	DBG_ASSERT( local_pavail().is_avail( piece_idx ) == false );

	// mark this piece_idx as isavail in bt_pselect_vapi_t
	pselect_vapi()->mark_isavail(piece_idx);	

	// notify all the current bt_swarm_full_t of that this piece is newly available
	std::list<bt_swarm_full_t *>::iterator	iter;
	for(iter = full_db.begin(); iter != full_db.end(); iter++){
		bt_swarm_full_t *	swarm_full = *iter;
		// notify this piece_idx as isavail in this bt_swarm_full_t
		swarm_full->declare_piece_newly_avail(piece_idx);
	}
	// NOTE: no need to notify the bt_ecnx_vapi_t connection as they dont
	// export the piece locally available

	// forward to bt_utmsg_piecewish_t IIF supported by the local peer
	bt_utmsg_piecewish_t *	utmsg_piecewish	= swarm_utmsg()->utmsg_piecewish();
	if( utmsg_piecewish )	utmsg_piecewish->declare_piece_newly_avail(piece_idx); 

	// notify the caller via a delayed bt_swarm_event_t
	return notify_callback(bt_swarm_event_t::build_piece_newly_avail(piece_idx));
}

/** \brief called to notify a piece is nomore locally available
 * 
 * - NOTE:
 *   - it is only called from the outside of the bt_swarm_t
 *   - e.g. when a piece is no more needed and is choosed to be removed, to free
 *     space.
 *   - THIS is doing the delete data too
 * - TODO wow this is shitty code. like ultra shitty
 *   - how to explain the fact that nomore_avail dont notify event
 *   - while newly_avail does notify event
 *   - moreover it cause trouble of nested notification in casti/casto because 
 *     the same object call this function and receive the bt_swarm_t event
 *     - i cant find it back but i remember seeing this, maybe in oload ? 
 *     - if so, do zerotime to delete the piece
 *   - because newly_avail may be internal while nomore_avail is not internal
 *   - on the other hand newly_avail may be external too 
 *     - e.g. in the neoip-casti
 */
bt_swarm_t &	bt_swarm_t::declare_piece_nomore_avail(size_t piece_idx)	throw()
{
	// sanity check - the piece MUST be currently available
	DBG_ASSERT( pselect_vapi()->isavail( piece_idx ) );
	// sanity check - this piece_idx MUST be NONEED in bt_pselect_vapi_t
	DBG_ASSERT( pselect_vapi()->pieceprec(piece_idx).is_notneeded() );

	// mark this piece_idx as isavail in bt_pselect_vapi_t
	pselect_vapi()->mark_unavail(piece_idx);	

	// remove the data for this piece_idx in the bt_io_vapi
	bt_err_t	bt_err;
	bt_err		= io_vapi()->remove( bt_unit_t::pieceidx_to_totfile_range(piece_idx, bt_mfile) );
	DBG_ASSERT( bt_err.succeed() );	// TODO poor error management - what to do in case of error ?
					// TODO notify a DISK_ERROR

	// notify it to all bt_swarm_full_t
	// - full_db is copied as swarm_full->declare_piece_nomore_avail may do autodelete
	std::list<bt_swarm_full_t *>		full_db_copy	= full_db;
	std::list<bt_swarm_full_t *>::iterator	iter;
	for(iter = full_db_copy.begin(); iter != full_db_copy.end(); iter++){
		bt_swarm_full_t *	swarm_full = *iter;
		// notify this piece_idx as unavail in this bt_swarm_full_t
		swarm_full->declare_piece_nomore_avail(piece_idx);
	}

	// forward to bt_utmsg_piecewish_t IIF supported by the local peer
	bt_utmsg_piecewish_t *	utmsg_piecewish	= swarm_utmsg()->utmsg_piecewish();
	if( utmsg_piecewish )	utmsg_piecewish->declare_piece_nomore_avail(piece_idx); 

	// return the object itself
	return *this;
}

/** \brief notify a DISK_ERROR
 * 
 * - NOTE: this function may delete the bt_swarm_t 
 *   - in fact this function notify the caller of the event and the caller
 *     may delete the bt_swarm_t
 * 
 * @return a tokeep for the bt_swarm_t
 */
bool	bt_swarm_t::notify_disk_error(const bt_err_t &bt_err)		throw()
{
	// notify the caller
	return notify_callback( bt_swarm_event_t::build_disk_error(bt_err) );
}


/** \brief Function used to update the listen_pview *DURING* bt_session_t run
 */
void	bt_swarm_t::update_listen_pview(const ipport_addr_t &new_listen_pview)	throw()
{
	// log to debug
	KLOG_ERR("enter NOT YET IMPLEMENTED new_listen_pview=" << new_listen_pview);
	// TODO update the required stuff
	// - e.g. i can think about the utmsg handshake
	// - there are likely other... to check	


	// go thru the whole full_db
	std::list<bt_swarm_full_t *>::iterator	iter;
	for(iter = full_db.begin(); iter != full_db.end(); iter++){
		bt_swarm_full_t *	swarm_full = *iter;
		// notify this bt_swarm_full_t of the new_listen_pview
		swarm_full->update_listen_pview(new_listen_pview);
	}
}

////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////
//                     main notification function
////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

/** \brief notify the callback with the tcp_event
 */
bool bt_swarm_t::notify_callback(const bt_swarm_event_t &event)	throw()
{
	// sanity check - the callback MUST NOT be NULL
	DBG_ASSERT( callback );
	// backup the tokey_check_t context to check after the callback notification
	TOKEEP_CHECK_BACKUP_DFL(*callback);
	// notify the caller
	bool tokeep = callback->neoip_bt_swarm_cb(userptr, *this, event);
	// sanity check - tokeep MUST be false if the local object has been deleted, true otherwise 
	TOKEEP_CHECK_MATCH_DFL(tokeep);
	// return the tokeep
	return tokeep;
}

NEOIP_NAMESPACE_END





